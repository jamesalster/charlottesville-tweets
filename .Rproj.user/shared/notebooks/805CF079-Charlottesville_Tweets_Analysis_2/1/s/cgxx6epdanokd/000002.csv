"0",""
"0","##  Language and Word Groups ##"
"0",""
"0","groups = c(""white supremacis"","
"0","           ""white nationalis"","
"0","           ""klu klux klan"","
"0","           ""black lives matter"","
"0","           ""both sides"","
"0","           ""fake news"","
"0","           ""fox news"","
"0","           ""white house"","
"0","           ""the people"","
"0","           ""press conference"") "
"0",""
"0","make_groupings = function(text_vec, groups) {"
"0","  gp_rx = str_c(""(?i)"", str_c(groups, collapse = ""|""))"
"0","  str_replace_all(text_vec, gp_rx, function(x) str_replace_all(x, ""\\s"", ""_""))"
"0","}"
"0",""
"0","tweets = raw_tweets %>%"
"0","  filter(detect_language(full_text) == ""en"") %>%  #only english"
"0","  mutate(full_text = make_groupings(full_text, groups)) #group important word "
"0",""
"0","## Tokenise ##"
"0",""
"0","by_word = tweets %>%   "
"0","  unnest_tokens(word, full_text, token = ""regex"","
"0","                pattern = ""[[:punct:][:space:]-[\\'/#@_-]]"", to_lower = TRUE) %>%"
"0","  filter(!str_detect(word, ""^(https|t|www|s|//t|n|[0-9]|-|\\||via)$""), #no url bits, which we've broken up"
"0","         !str_detect(word, ""^co/""), #ditto"
"0","         !str_detect(word, ""[^[:ascii:]]""),  #no emojis"
"0","         !str_detect(word, ""^amp$""), #no amp"
"0","         !str_detect(word, ""charlottesville"")) %>% #no charlottesville"
"0","  anti_join(get_stopwords('en')) #no stopwords"
"0",""
