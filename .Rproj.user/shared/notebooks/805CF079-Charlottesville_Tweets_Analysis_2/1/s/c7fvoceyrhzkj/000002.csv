"0",""
"0","### topic model ###"
"0",""
"0","#(just load from file because it takes a long time to do)"
"0","if (FALSE) {"
"0","  #get top 100 words by count"
"0","  top_words = by_word %>%"
"0","    filter(!str_detect(word, ""^(#|@)"")) %>% #no ats or hashtags"
"0","    filter(!word %in% c(""trump"", ""trump's"", ""via"")) %>%"
"0","    count(word) %>%"
"0","    slice_max(n, n = 100) %>%"
"0","    pull(word)"
"0","  "
"0","  #make document-term-matrix"
"0","  dtm = by_word %>%"
"0","    filter(word %in% top_words) %>%"
"0","    count(id, word) %>%"
"0","    cast_dtm(id, word, n)"
"0","  "
"0","  #model"
"0","  topic_mod = LDA(dtm, k = 6, control = list(seed = 234932, alpha = 0.05))"
"0","} else topic_mod = read_rds(""new_topic_mod_very_low_alpha.rds.gz"")"
"0",""
"0","#get the topic a tweet is most likely to belong to"
"0","twts_by_topic = tidy(topic_mod, matrix = ""gamma"") %>%"
"0","  with_groups(document, ~ slice_max(., gamma, n = 1)) %>%"
"0","  filter(gamma > 0.5)"
"0",""
