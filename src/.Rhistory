library(tidyverse)
library(scales)
library(lubridate)
library(colorspace)
#text
library(cld2)
library(tidytext)
library(fuzzyjoin)
library(topicmodels)
#networks
library(widyr)
library(tidygraph)
library(ggraph)
library(patchwork)
raw_tweets = tibble(filename = str_c("../data/aug", 15:17, "_sample.csv.zip")) %>%
rowwise() %>%
summarise(filename = filename,
read_csv(filename))
groups = c("white supremacis",
"white nationalis",
"klu klux klan",
"black lives matter",
"both sides",
"fake news",
"fox news",
"white house",
"the people",
"press conference")
make_groupings = function(text_vec, groups) {
gp_rx = str_c(groups, collapse = "|")
str_replace_all(text_vec, gp_rx, function(x) str_replace_all(x, "\\s", "_"))
}
tweets = raw_tweets %>%
filter(detect_language(full_text) == "en") %>%  #only english
mutate(full_text = make_groupings(full_text, groups)) #group important word sequences
by_word = tweets %>%
unnest_tokens(word, full_text, token = "regex",
pattern = "[[:punct:][:space:]-[\\'/#@_-]]", to_lower = TRUE) %>% #move to one-word-per-row
filter(!str_detect(word, "^(https|t|www|s|//t|n|[0-9]|-)$"), #no url bits, which we've broken up
!str_detect(word, "^co/"), #ditto
!str_detect(word, "[^[:ascii:]]"),  #no emojis
!str_detect(word, "^amp$"), #no amp
!str_detect(word, "charlottesville")) %>% #no charlottesville
anti_join(get_stopwords('en')) #no stopwords/
knitr::opts_chunk$set(echo = FALSE,
warning = FALSE,
message = FALSE,
cache = TRUE,
fig.width = 10,
fig.asp = 5/7)
#get eastern standard time
raw_tweets_est = raw_tweets %>%
mutate(created_at_est = with_tz(created_at, "US/Eastern"),
filename = str_remove(filename, "data/"))
#get eastern standard time
raw_tweets_est = raw_tweets %>%
mutate(created_at_est = with_tz(created_at, "US/Eastern"),
filename = str_remove(filename, "data/"))
#find last tweet in each file
last_twts = raw_tweets_est %>%
group_by(filename) %>%
summarise(time = max(created_at))
#plot
ggplot(raw_tweets_est, aes(x = created_at_est, fill = filename)) +
#..count.. exposes number per unit of x (here, per second).
#adjust adjusts the bin size for smoothing
geom_density(aes(y = ..count.. * 60), adjust = 1/2) +
geom_vline(data = last_twts, aes(xintercept = time), linetype = "dashed") +
scale_x_datetime(breaks = breaks_pretty(8),
labels = label_date_short(format = c("", "", "%b %d", "%H:%M"))) +
scale_y_continuous(name = "tweets per minute") +
labs(x = "time created (Eastern Standard Time)",
title = "Distribution of tweets by time created",
subtitle = str_glue("n = {nrow(raw_tweets_est)}, {nrow(raw_tweets_est) / 3} per file")) +
scale_fill_discrete_qualitative(palette = "Harmonic") +
theme_minimal() +
theme(legend.position = "top")
raw_tweets %>%
distinct(user_id, .keep_all = TRUE) %>%
mutate(time_zone = fct_collapse(user_time_zone,
"Other US" = c("America/Chicago",
"America/Detroit",
"America/Los_Angeles",
"America/New_York",
"Arizona",
"CET", "CST", "EST", "EDT", "Hawaii", "GMT-6", "Pacific/Honolulu")),
time_zone = fct_other(time_zone,
keep = c(str_subset(levels(time_zone), "US"), "null"),
other_level = "Rest of World")) %>%
tabyl(time_zone) %>%
arrange(desc(n)) %>%
adorn_totals('row') %>%
adorn_pct_formatting(digits = 1) %>%
kable(caption = "Table 1: User timezones")
library(knitr)
raw_tweets %>%
distinct(user_id, .keep_all = TRUE) %>%
mutate(time_zone = fct_collapse(user_time_zone,
"Other US" = c("America/Chicago",
"America/Detroit",
"America/Los_Angeles",
"America/New_York",
"Arizona",
"CET", "CST", "EST", "EDT", "Hawaii", "GMT-6", "Pacific/Honolulu")),
time_zone = fct_other(time_zone,
keep = c(str_subset(levels(time_zone), "US"), "null"),
other_level = "Rest of World")) %>%
tabyl(time_zone) %>%
arrange(desc(n)) %>%
adorn_totals('row') %>%
adorn_pct_formatting(digits = 1) %>%
kable(caption = "Table 1: User timezones")
library(janitor)
library(lubridate)
#networks
library(widyr)
library(tidygraph)
library(ggraph)
raw_tweets %>%
distinct(user_id, .keep_all = TRUE) %>%
mutate(time_zone = fct_collapse(user_time_zone,
"Other US" = c("America/Chicago",
"America/Detroit",
"America/Los_Angeles",
"America/New_York",
"Arizona",
"CET", "CST", "EST", "EDT", "Hawaii", "GMT-6", "Pacific/Honolulu")),
time_zone = fct_other(time_zone,
keep = c(str_subset(levels(time_zone), "US"), "null"),
other_level = "Rest of World")) %>%
tabyl(time_zone) %>%
arrange(desc(n)) %>%
adorn_totals('row') %>%
adorn_pct_formatting(digits = 1) %>%
kable(caption = "Table 1: User timezones")
#get tweets per user
user_tweets = raw_tweets %>%
count(user_id, name = "n_tweets")
#get tweets per user
user_tweets = raw_tweets %>%
count(user_id, name = "n_tweets")
#calculate confidence interval
n_twt_conf = user_tweets %>%
summarise(mean = mean(n_tweets),
sd = sd(n_tweets),
n_users = n()) %>%
mutate(error = qnorm(0.975) * sd / sqrt(n_users),
lower = mean - error,
upper = mean + error,
across(everything(), ~ round(., 2)))
#get top user
top_user = user_tweets %>%
right_join(raw_tweets, by = "user_id") %>%
filter(n_tweets == max(n_tweets)) %>%
pull(user_name) %>%
pluck(1)
#get percentage of tweeters tweeted by those who tweet more than 5 times
pct_over_5 = user_tweets %>%
group_by(over_5 = n_tweets > 5) %>%
summarise(tot = sum(n_tweets)) %>%
mutate(pct = round(tot / sum(tot) * 100, 1)) %>%
filter(over_5) %>% pull(pct)
#tabulate tweets per user
user_tweets %>%
mutate(n_tweets = fct_other(as.character(n_tweets),
keep = as.character(1:5),
other_level = ">5")) %>%
tabyl(n_tweets) %>%
adorn_totals('row') %>%
adorn_pct_formatting(1) %>%
kable(caption = "Table 2: Number of tweets per user")
#load random sample
random_users = read_rds("../resources/random_tweets.rds.gz") %>%
select(user_id, followers_count) %>%
distinct(user_id, .keep_all = TRUE) %>%
slice_sample(n = 9000) %>%
mutate(across(everything(), as.numeric),
dataset = "unfiltered stream")
#load random sample
random_users = read_rds("../resources/random_tweets.csv") %>%
select(user_id, followers_count) %>%
distinct(user_id, .keep_all = TRUE) %>%
slice_sample(n = 9000) %>%
mutate(across(everything(), as.numeric),
dataset = "unfiltered stream")
#load random sample
random_users = read_csv("../resources/random_tweets.csv") %>%
select(user_id, followers_count) %>%
distinct(user_id, .keep_all = TRUE) %>%
slice_sample(n = 9000) %>%
mutate(across(everything(), as.numeric),
dataset = "unfiltered stream")
#load user_ids from dataset
user_sample = raw_tweets %>%
select(user_id, followers_count) %>%
distinct(user_id, .keep_all = TRUE) %>%
mutate(dataset = "charlottesville")
bind_rows(user_sample, random_users) %>%
with_groups(dataset, ~ mutate(., mean = mean(followers_count))) %>%
mutate(across(is.numeric, ~ log10(. + 1))) %>%
ggplot(aes(x = followers_count, fill = dataset, colour = dataset)) +
geom_density(alpha = 0.5) +
labs(title = "Distribution of users by # of followers (log transformed)",
x = "log10(1 + followers_count)",
y = "Density") +
scale_fill_discrete_qualitative(palette = "Harmonic") +
theme_minimal() +
theme(legend.position = "top",
strip.text = element_text(face = "bold", size = 10))
#load random sample
random_users = read_rds("../resources/random_tweets.rds.gz") %>%
select(user_id, followers_count) %>%
distinct(user_id, .keep_all = TRUE) %>%
slice_sample(n = 9000) %>%
mutate(across(everything(), as.numeric),
dataset = "unfiltered stream")
#load user_ids from dataset
user_sample = raw_tweets %>%
select(user_id, followers_count) %>%
distinct(user_id, .keep_all = TRUE) %>%
mutate(dataset = "charlottesville")
bind_rows(user_sample, random_users) %>%
with_groups(dataset, ~ mutate(., mean = mean(followers_count))) %>%
mutate(across(is.numeric, ~ log10(. + 1))) %>%
ggplot(aes(x = followers_count, fill = dataset, colour = dataset)) +
geom_density(alpha = 0.5) +
labs(title = "Distribution of users by # of followers (log transformed)",
x = "log10(1 + followers_count)",
y = "Density") +
scale_fill_discrete_qualitative(palette = "Harmonic") +
theme_minimal() +
theme(legend.position = "top",
strip.text = element_text(face = "bold", size = 10))
knitr::opts_chunk$set(echo = FALSE,
warning = FALSE,
message = FALSE,
cache = TRUE,
fig.width = 10,
fig.asp = 5/7,
fig.topcaption = TRUE)
#load random sample
random_users = read_rds("../resources/random_tweets.rds.gz") %>%
select(user_id, followers_count) %>%
distinct(user_id, .keep_all = TRUE) %>%
slice_sample(n = 9000) %>%
mutate(across(everything(), as.numeric),
dataset = "unfiltered stream")
#load user_ids from dataset
user_sample = raw_tweets %>%
select(user_id, followers_count) %>%
distinct(user_id, .keep_all = TRUE) %>%
mutate(dataset = "charlottesville")
#plot density graph
bind_rows(user_sample, random_users) %>%
with_groups(dataset, ~ mutate(., mean = mean(followers_count))) %>%
mutate(across(where(is.numeric), ~ log10(. + 1))) %>%
ggplot(aes(x = followers_count, fill = dataset, colour = dataset)) +
geom_density(alpha = 0.5) +
labs(title = "Distribution of users by # of followers (log transformed)",
x = "log10(1 + followers_count)",
y = "Density") +
scale_fill_discrete_qualitative(palette = "Harmonic") +
theme_minimal() +
theme(legend.position = "top",
strip.text = element_text(face = "bold", size = 10))
#get hashtags
hashtags = by_word %>%
filter(str_detect(word, "^#"))
#get top 20 hashtags
top_hashtags = hashtags %>%
count(word) %>%
slice_max(n, n = 20) %>%
pull(word)
#plot as network of co-occurrences
set.seed(45480)
hashtags %>%
filter(word %in% top_hashtags) %>%
pairwise_count(word, id, upper = FALSE) %>% #count co-occurrences by tweet
tbl_graph(edges = ., directed = FALSE) %>% #make network
activate(edges) %>%
filter(n > 20) %>% #only > 20 co-occurrences
activate(nodes) %>%
filter(local_size() > 1) %>% #only ones connected to the network
ggraph(layout = "fr", weights = .data$n, niter = 1e6) +
geom_edge_link(aes(edge_colour = n), width = 1.5, alpha = 0.5) +
geom_node_text(aes(label = name, size = local_size())) +
labs(title = "Co-occurrence of top 20 hashtags",
subtitle = "> 20 co-occurences and > 1 neighbour only",
caption = "Layout algorithm:  fr") +
scale_x_continuous(expand = c(0.05, 0.05)) +
scale_edge_colour_gradientn(name = "number of\nco-occurences",
colours = sequential_hcl(palette = "Terrain", n = 5, rev = TRUE)) +
scale_size_continuous(range = c(4, 5), name = "number of\nneighbours") +
theme_classic()
print_tweets = function(tweet_rows) {
twt_brk = str_c(rep('-', 50), collapse = "")
tweet_rows %>%
mutate(across(where(is.character), ~ str_remove_all(., "\\n"))) %>%
with(print(str_glue("\n{twt_brk}\n{user_name} | @{screen_name} | {created_at %>% with_tz('US/Eastern') %>% strftime('%d %b %Y %R')} EST\n{str_wrap(full_text, indent = 3, exdent = 3, width = 80)}")))
cat(twt_brk)
}
show_pairs = function(word1, word2, n = 3, seed = 1234) {
set.seed(seed)
tweets %>%
filter(str_detect(full_text, str_c("(?i)", word1)),
str_detect(full_text, str_c("(?i)", word2))) %>%
slice_sample(n = n) %>%
print_tweets()
}
``` {r vis_impeach_trump}
show_pairs("#impeachtrump", "#maga", seed = 226)
#get top 10 @s
top_ats = by_word %>%
filter(str_detect(word, "^@")) %>%
count(word, name = "n_at") %>%
slice_max(n_at, n = 10)
#get top 10 @s
top_ats = by_word %>%
filter(str_detect(word, "^@")) %>%
count(word, name = "n_at") %>%
slice_max(n_at, n = 10)
#count associations to top ats
top_at_assocs = by_word %>%
pairwise_count(word, id) %>%
inner_join(top_ats, by = c("item1" = "word")) %>% #filtering join
filter(!str_detect(item2, "^@"))
#count associations to top ats
top_at_assocs = by_word %>%
pairwise_count(word, id) %>%
inner_join(top_ats, by = c("item1" = "word")) %>% #filtering join
filter(!str_detect(item2, "^@"))
#plot as network
set.seed(3990)
#plot as network
set.seed(3990)
top_at_assocs %>%
with_groups(item1, ~ slice_max(., n, n = 30)) %>% #get top 30 associations
tbl_graph(edges = .) %>% #make network
mutate(is_at = str_detect(name, "^@")) %>% #define @s
filter(local_size() > 2) %>% # > 1 neighbour
ggraph(layout = "fr" , weights = .data$n, niter = 1e6) +
geom_edge_link(aes(edge_colour = n), alpha = 0.5, width = 1.5) +
geom_node_text(aes(label = name, size = is_at), alpha = 0.7) +
labs(title = "Co-occurrence of concepts with top 10 @s",
subtitle = "> 1 neighbour only",
caption = "Layout algorithm:  fr") +
scale_edge_colour_gradientn(name = "number of\nco-occurences ",
trans = log10_trans(),
colours = sequential_hcl(palette = "Terrain",
n = 5, rev = TRUE)) +
scale_size_discrete(range = c(3.5, 5), guide = FALSE) +
theme_classic()
dict = read_csv("../resources/actors_dict.csv")
#takes a while so load a pre-coded one
if (FALSE) {
dict_coded = regex_full_join(by_word, dict, by = c("word" = "word_rx"))
write_rds(dict_coded, "twts_coded.rds.gz", compress = "gz")
} else dict_coded = read_rds("twts_coded.rds.gz")
#takes a while so load a pre-coded one
dict_coded = regex_full_join(by_word, dict, by = c("word" = "word_rx"))
#takes a while so load a pre-coded one
if (FALSE) {
dict_coded = regex_full_join(by_word, dict, by = c("word" = "word_rx"))
write_rds(dict_coded, "../resources/twts_coded.rds.gz", compress = "gz")
} else dict_coded = read_rds("../resources/twts_coded.rds.gz")
contrast = c("denounce", "denounces", "denouncing", "denounced",
"condemn", "condemns", "condemning", "condemned",
"attack", "attacks", "attacking", "attacked",
"slam", "slams", "slamming", "slammed",
#"blame", "blames", "blaming", "blamed", avoid 'blame both sides'
"^hate", "^hates", "hating", "^hated",
"^sue$", "^sues$", "suing", "sued",
"disown", "disowns", "disowning", "disowned",
"respond", "responding", "responds", "responded",
"call out", "calls out", "calling out", "called out",
#others:
"response", "against", "but", "however", "while", "although"
)
#get ids of all tweets that contain the contrast words
contrast_ids = tweets %>%
filter(str_detect(full_text, str_c(contrast, collapse = "|"))) %>%
pull(id)
#get ids of all tweets that contain the contrast words
contrast_ids = tweets %>%
filter(str_detect(full_text, str_c(contrast, collapse = "|"))) %>%
pull(id)
#helper function to plot mds
#takes grouped output of widyr::pairwise_() via dplyr::group_map(), returns mds plot
mds_plot = function(simil_df, group_info, text_size = 5) {
#get title from group name
plot_title = unlist(group_info) %>% as.character()
#make wide form
simil_square = simil_df %>%
pivot_wider(names_from = item2,
values_from = n) %>%
relocate(item1, pull(., item1))  #get rows and columns in same order
#transform wide form into a distance matrix
simil_mat = simil_square %>%
select(-item1) %>%
as.matrix()
#replace NAs in matrix to avoid plotting error
diag(simil_mat) = 1
simil_mat = replace(simil_mat, is.na(simil_mat), 0)
#create the network and plot
create_complete(nrow(simil_mat)) %>%
mutate(name = simil_square$item1) %>%
ggraph(layout = "mds", dist = simil_mat) +
geom_node_text(aes(label = name), size = text_size, fontface = "bold",
alpha = 0.7) +
geom_vline(xintercept = 0, linetype = "dashed") +
geom_hline(yintercept = 0, linetype = "dashed") +
scale_x_continuous(expand = c(0.1, 0.1)) +
scale_y_continuous(expand = c(0.1, 0.1)) +
labs(title = plot_title) +
theme_bw()
}
#plot tweets with contrasting words, and everything
dict_coded %>%
filter(!is.na(assoc),
assoc_group != "Pronoun") %>%
group_by(only_contrast = if_else(id %in% contrast_ids,
"Only 'contrasting' tweets", "Everything")) %>%
relocate(only_contrast, .after = last_col()) %>%
group_modify(~ pairwise_count(., assoc, id)) %>%
group_map(~ mds_plot(.x, .y)) %>%
wrap_plots(nrow = 1) +
plot_annotation(title = "Contrasted actors",
subtitle = "Distant actors co-occur the most often",
caption = "Layout algorithm: MDS")
## helper function to show word pairs from the dictionary ##
show_dict_pairs = function(assoc1, assoc2, n = 5, seed = 123, only_contrast = FALSE) {
#construct regex by association in dictionary
or_rx = function(an_assoc) {
dict %>%
filter(assoc == an_assoc) %>%
pull(word) %>%
str_replace("\\$", "") %>%
str_c(collapse = "\\W|\\W")
}
if (only_contrast) {
ctr_tweets = tweets %>% filter(str_detect(full_text, str_c(contrast, collapse = "|")))
}
else ctr_tweets = tweets
#visualise
set.seed(seed)
ctr_tweets %>%
filter(str_detect(full_text, or_rx(assoc1)),
str_detect(full_text, or_rx(assoc2))) %>%
slice_sample(n = n) %>%
print_tweets()
}
show_dict_pairs("Rep", "Trump", 3, 12233, only_contrast = TRUE)
